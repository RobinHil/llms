# ğŸ¤– Local / server LLMs API Deployment

This repository contains Docker Compose configurations for deploying local Large Language Models (LLMs) APIs using LocalAI and Ollama.

## ğŸŒŸ Features

- ğŸš€ Easy deployment with Docker Compose
- ğŸ® GPU support with NVIDIA CUDA
- ğŸ”„ Automatic model downloading
- ğŸ¥ Built-in health checks
- ğŸ”Œ Isolated network configuration

## ğŸ“‹ Prerequisites

- Docker and Docker Compose installed
- NVIDIA GPU with CUDA support
- NVIDIA Container Toolkit installed

## ğŸ¯ Default Models

### LocalAI
- Mistral 7B Instruct v0.3
- All-MiniLM-L6-v2 (for embeddings)
- Meta Llama 3.1 70B Instruct

### Ollama
- Mistral
- Nomic Embed Text (for embeddings)

## ğŸš€ Getting Started

1. Clone this repository:
```bash
git clone https://github.com/RobinHil/llms.git
cd llms
```

2. Start the services:
```bash
docker compose -p [compose_alias] up -d
```

## ğŸ”Œ Service Endpoints

### LocalAI
- Port: 8080
- Health Check: `http://localhost:8080/readyz`
- Volume: `localai_models`
- Network: `localai_network`

### Ollama
- Port: 11434
- Health Check: `http://localhost:11434`
- Volume: `ollama_data`
- Network: `ollama_network`

## ğŸ› ï¸ Configuration

Both services are configured with:
- Automatic GPU detection and utilization
- Persistent storage volumes
- Automatic model downloading on startup
- Health monitoring
- Automatic restart capability

## ğŸ—ï¸ Project Structure

```
.
â”œâ”€â”€ localai/
â”‚   â””â”€â”€ docker-compose.yml  # LocalAI configuration
â”œâ”€â”€ ollama/
â”‚   â”œâ”€â”€ docker-compose.yml  # Ollama configuration
â”‚   â””â”€â”€ entrypoint.sh       # Ollama startup script
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```

## ğŸ” Health Monitoring

Both services include health checks:
- LocalAI: Checks every 1 minute with a 20-minute timeout
- Ollama: Checks every 10 seconds with a 10-second timeout

## âš¡ Performance

For optimal performance, both services are configured to use all available NVIDIA GPUs through the NVIDIA Container Toolkit.
