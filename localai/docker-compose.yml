services:
  localai:
    image: localai/localai:latest-aio-gpu-nvidia-cuda-12
    container_name: localai
    environment:
      MODELS: "mistral-7b-instruct-v0.3,all-MiniLM-L6-v2,meta-llama-3.1-70b-instruct"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: ["gpu"]
              count: all
    ports:
      - "8080:8080"
    networks:
      - network
    restart: always
    volumes:
      - localai_models:/build/models
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/readyz"]
      interval: 1m
      timeout: 20m
      retries: 5

volumes:
  localai_models:
    name: localai_models

networks:
  network:
    driver: bridge
    name: localai_network